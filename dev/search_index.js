var documenterSearchIndex = {"docs":
[{"location":"tutorials/generalized_likelihood/#Generalized-Likelihood-Inference","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"","category":"section"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"In this example, we will demo the likelihood-based approach to parameter fitting. First, let's generate a dataset to fit. We will re-use the Lotka-Volterra equation, but in this case, fit just two parameters.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using DifferentialEquations, DiffEqParamEstim, Optimization, OptimizationBBO\nf1 = function (du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3.0 * u[2] + u[1] * u[2]\nend\np = [1.5, 1.0]\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\nprob1 = ODEProblem(f1, u0, tspan, p)\nsol = solve(prob1, Tsit5())","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This is a function with two parameters, [1.5,1.0] which generates the same ODE solution as before. This time, let's generate 100 datasets where at each point adds a little bit of randomness:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0, stop = 10, length = 200))\nfunction generate_data(sol, t)\n    randomized = VectorOfArray([(sol(t[i]) + 0.01randn(2)) for i in 1:length(t)])\n    data = convert(Array, randomized)\nend\naggregate_data = convert(Array, VectorOfArray([generate_data(sol, t) for i in 1:100]))","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"here, with t we measure the solution at 200 evenly spaced points. Thus, aggregate_data is a 2x200x100 matrix where aggregate_data[i,j,k] is the ith component at time j of the kth dataset. What we first want to do is get a matrix of distributions where distributions[i,j] is the likelihood of component i at take j. We can do this via fit_mle on a chosen distributional form. For simplicity, we choose the Normal distribution. aggregate_data[i,j,:] is the array of points at the given component and time, and thus we find the distribution parameters which fits best at each time point via:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Distributions\ndistributions = [fit_mle(Normal, aggregate_data[i, j, :]) for i in 1:2, j in 1:200]","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Notice for example that we have:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"distributions[1, 1]","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"that is, it fits the distribution to have its mean just about where our original solution was, and the variance is about how much noise we added to the dataset. This is a good check to see that the distributions we are trying to fit our parameters to makes sense.","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Note that in this case the Normal distribution was a good choice, and often it's a nice go-to choice, but one should experiment with other choices of distributions as well. For example, a TDist can be an interesting way to incorporate robustness to outliers since low degrees of free T-distributions act like Normal distributions but with longer tails (though fit_mle does not work with a T-distribution, you can get the means/variances and build appropriate distribution objects yourself).","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Once we have the matrix of distributions, we can build the objective function corresponding to that distribution fit:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"obj = build_loss_objective(prob1, Tsit5(), LogLikeLoss(t, distributions),\n    maxiters = 10000, verbose = false)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"First, let's use the objective function to plot the likelihood landscape:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"using Plots;\nplotly();\nprange = 0.5:0.1:5.0\nheatmap(prange, prange, [obj([j, i]) for i in prange, j in prange],\n    yscale = :log10, xlabel = \"Parameter 1\", ylabel = \"Parameter 2\",\n    title = \"Likelihood Landscape\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 2 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"Recall that this is the negative log-likelihood, and thus the minimum is the maximum of the likelihood. There is a clear valley where the first parameter is 1.5, while the second parameter's likelihood is more muddled. By taking a one-dimensional slice:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"plot(prange, [obj([1.5, i]) for i in prange], lw = 3,\n    title = \"Parameter 2 Likelihood (Parameter 1 = 1.5)\",\n    xlabel = \"Parameter 2\", ylabel = \"Objective Function Value\")","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"(Image: 1 Parameter Likelihood)","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"we can see that there's still a clear minimum at the true value. Thus, we will use the global optimizers from BlackBoxOptim.jl to find the values. We set our search range to be from 0.5 to 5.0 for both of the parameters and let it optimize:","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"bound1 = Tuple{Float64, Float64}[(0.5, 5), (0.5, 5)]\noptprob = OptimizationProblem(obj, [2.0, 2.0], lb = first.(bound1), ub = last.(bound1))","category":"page"},{"location":"tutorials/generalized_likelihood/","page":"Generalized Likelihood Inference","title":"Generalized Likelihood Inference","text":"This shows that it found the true parameters as the best fit to the likelihood.","category":"page"},{"location":"methods/collocation_loss/#Two-Stage-method-(Non-Parametric-Collocation)","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"","category":"section"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"The two-stage method is a collocation method for estimating parameters without requiring repeated solving of the differential equation. It does so by determining a smoothed estimated trajectory of the data (local quadratic polynomial fit by least squares) and optimizing the derivative function and the data's timepoints to match the derivatives of the smoothed trajectory. This method has less accuracy than other methods but is much faster, and is a good method to try first to get in the general “good parameter” region, to then finish using one of the other methods.","category":"page"},{"location":"methods/collocation_loss/","page":"Two Stage method (Non-Parametric Collocation)","title":"Two Stage method (Non-Parametric Collocation)","text":"function two_stage_objective(prob::DEProblem, tpoints, data, adtype = SciMLBase.NoAD(), ;\n        kernel = :Epanechnikov,\n        loss_func = L2DistLoss)\nend","category":"page"},{"location":"getting_started/#Getting-Started-with-Optimization-Based-ODE-Parameter-Estimation","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"In this tutorial, we will showcase how to estimate the parameters of an ordinary differential equation using DiffEqParamEstim.jl. DiffEqParamEstim.jl is a high-level tool that makes common parameter estimation tasks simple. Here, we will show how to use its cost function generation to estimate the parameters of the Lotka-Volterra equation against simulated data.","category":"page"},{"location":"getting_started/#Installation","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Installation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"First, we will make sure DiffEqParamEstim.jl is installed correctly. To do this, we use the Julia package REPL, opened by typing ] in the REPL and seeing pkg> appear in blue. Then we type: add DiffEqParamEstim and hit enter. This will run the package management sequence, and we will be good to go.","category":"page"},{"location":"getting_started/#Required-Dependencies","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Required Dependencies","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Module Description\nDifferentialEquations.jl The numerical differential equation solver package\nRecursiveArrayTools.jl Tooling for recursive arrays like vector of arrays\nPlots.jl Tooling for plotting and visualization\nZygote.jl Tooling for reverse-mode automatic differentiation (gradient calculations)\nOptimization.jl The numerical optimization package\nOptimizationOptimJL.jl The Optim optimizers we will use for local optimization\nOptimizationBBO.jl The BlackBoxOptim optimizers we will use for global optimization","category":"page"},{"location":"getting_started/#Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Parameter Estimation in the Lotka-Volterra Equation: 1 Parameter Case","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We choose to optimize the parameters on the Lotka-Volterra equation. Let's start by defining the equation as a function with parameters:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"using DifferentialEquations, RecursiveArrayTools, Plots, DiffEqParamEstim\nusing Optimization, ForwardDiff, OptimizationOptimJL, OptimizationBBO\n\nfunction f(du, u, p, t)\n    du[1] = dx = p[1] * u[1] - u[1] * u[2]\n    du[2] = dy = -3 * u[2] + u[1] * u[2]\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5]\nprob = ODEProblem(f, u0, tspan, p)","category":"page"},{"location":"getting_started/#Generating-Synthetic-Data","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Generating Synthetic Data","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We create data using the numerical result with a=1.5:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"sol = solve(prob, Tsit5())\nt = collect(range(0, stop = 10, length = 200))\nusing RecursiveArrayTools # for VectorOfArray\nrandomized = VectorOfArray([(sol(t[i]) + 0.01randn(2)) for i in 1:length(t)])\ndata = convert(Array, randomized)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Here, we used VectorOfArray from RecursiveArrayTools.jl to turn the result of an ODE into a matrix.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"If we plot the solution with the parameter at a=1.42, we get the following:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"newprob = remake(prob, p = [1.42])\nnewsol = solve(newprob, Tsit5())\nplot(sol)\nplot!(newsol)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Notice that after one period, this solution begins to drift very far off: this problem is sensitive to the choice of a.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"To build the objective function for Optim.jl, we simply call the build_loss_objective function:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob, Tsit5(), L2Loss(t, data),\n    Optimization.AutoForwardDiff(),\n    maxiters = 10000, verbose = false)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set maxiters in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set verbose=false because this divergence can get noisy. The Optimization.AutoForwardDiff() is a choice of automatic differentiation, i.e., how the gradients are calculated. For more information on this choice, see the automatic differentiation choice API.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"note: Note\nA good rule of thumb is to use Optimization.AutoForwardDiff() for less than 100 parameters + states, and Optimization.AutoZygote() for more.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Before optimizing, let's visualize our cost function by plotting it for a range of parameter values:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"vals = 0.0:0.1:10.0\nplot(vals, [cost_function(i) for i in vals], yscale = :log10,\n    xaxis = \"Parameter\", yaxis = \"Cost\", title = \"1-Parameter Cost Function\",\n    lw = 3)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Now we can use the BFGS algorithm to optimize the parameter starting at a=1.42. We do this by creating an optimization problem and solving that with BFGS():","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"optprob = Optimization.OptimizationProblem(cost_function, [1.42])\noptsol = solve(optprob, BFGS())","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Now let's see how well the fit performed:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"newprob = remake(prob, p = optsol.u)\nnewsol = solve(newprob, Tsit5())\nplot(sol)\nplot!(newsol)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Note that some algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the documentation for Optim.jl.","category":"page"},{"location":"getting_started/#Adding-Bounds-Constraints","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Adding Bounds Constraints","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus, following the Optimization.jl documentation we can add box constraints to ensure the optimizer only checks between 0.0 and 3.0 which improves the efficiency of our algorithm. We pass the lb and ub keyword arguments to the OptimizationProblem to pass these bounds to the optimizer:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"lower = [0.0]\nupper = [3.0]\noptprob = Optimization.OptimizationProblem(cost_function, [1.42], lb = lower, ub = upper)\nresult = solve(optprob, BFGS())","category":"page"},{"location":"getting_started/#Estimating-Multiple-Parameters-Simultaneously","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Estimating Multiple Parameters Simultaneously","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let's use the Lotka-Volterra equation with all parameters free:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"function f2(du, u, p, t)\n    du[1] = dx = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = dy = -p[3] * u[2] + p[4] * u[1] * u[2]\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5, 1.0, 3.0, 1.0]\nprob = ODEProblem(f2, u0, tspan, p)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We can build an objective function and solve the multiple parameter version just as before:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob, Tsit5(), L2Loss(t, data),\n    Optimization.AutoForwardDiff(),\n    maxiters = 10000, verbose = false)\noptprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])\nresult_bfgs = solve(optprob, BFGS())","category":"page"},{"location":"getting_started/#Alternative-Cost-Functions-for-Increased-Robustness","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Alternative Cost Functions for Increased Robustness","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"The build_loss_objective with L2Loss is the most naive approach for parameter estimation. There are many others.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We can also use First-Differences in L2Loss by passing the kwarg differ_weight which decides the contribution of the differencing loss to the total loss.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"cost_function = build_loss_objective(prob, Tsit5(),\n    L2Loss(t, data, differ_weight = 0.3,\n        data_weight = 0.7),\n    Optimization.AutoForwardDiff(),\n    maxiters = 10000, verbose = false)\noptprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])\nresult_bfgs = solve(optprob, BFGS())","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"We can also use Multiple Shooting method by creating a multiple_shooting_objective","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"function ms_f1(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3.0 * u[2] + u[1] * u[2]\nend\nms_u0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\nms_p = [1.5, 1.0]\nms_prob = ODEProblem(ms_f1, ms_u0, tspan, ms_p)\nt = collect(range(0, stop = 10, length = 200))\ndata = Array(solve(ms_prob, Tsit5(), saveat = t, abstol = 1e-12, reltol = 1e-12))\nbound = Tuple{Float64, Float64}[(0, 10), (0, 10), (0, 10), (0, 10),\n    (0, 10), (0, 10), (0, 10), (0, 10),\n    (0, 10), (0, 10), (0, 10), (0, 10),\n    (0, 10), (0, 10), (0, 10), (0, 10), (0, 10), (0, 10)]\n\nms_obj = multiple_shooting_objective(ms_prob, Tsit5(), L2Loss(t, data),\n    Optimization.AutoForwardDiff();\n    discontinuity_weight = 1.0, abstol = 1e-12,\n    reltol = 1e-12)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"This creates the objective function that can be passed to an optimizer, from which we can then get the parameter values and the initial values of the short time periods, keeping in mind the indexing. Now we mix this with a global optimization method to improve robustness even more:","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"optprob = Optimization.OptimizationProblem(ms_obj, zeros(18), lb = first.(bound),\n    ub = last.(bound))\noptsol_ms = solve(optprob, BBO_adaptive_de_rand_1_bin_radiuslimited(), maxiters = 10_000)","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"optsol_ms.u[(end - 1):end]","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"Here as our model had 2 parameters, we look at the last 2 indexes of result to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section. We can also use a gradient-based optimizer with the multiple shooting objective.","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"optsol_ms = solve(optprob, BFGS())\noptsol_ms.u[(end - 1):end]","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"The objective function for the Two Stage method can be created and passed to an optimizer as","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"two_stage_obj = two_stage_objective(ms_prob, t, data, Optimization.AutoForwardDiff())\noptprob = Optimization.OptimizationProblem(two_stage_obj, [1.3, 0.8, 2.8, 1.2])\nresult = solve(optprob, Optim.BFGS())","category":"page"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"The default kernel used in the method is Epanechnikov, available others are Uniform,  Triangular, Quartic, Triweight, Tricube, Gaussian, Cosine, Logistic and Sigmoid, this can be passed by the kernel keyword argument. loss_func keyword argument can be used to pass the loss function (cost function) you want to use and passing a valid adtype argument enables Auto Differentiation.","category":"page"},{"location":"getting_started/#Conclusion","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Conclusion","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started with Optimization-Based ODE Parameter Estimation","title":"Getting Started with Optimization-Based ODE Parameter Estimation","text":"There are many more choices for how to improve the robustness of a parameter estimation. With all of these tools, one likely should never do the simple “solve it with p and check the L2 loss”. Instead, we should use these tricks to improve the loss landscape and increase the ability for optimizers to find globally the best parameters.","category":"page"},{"location":"tutorials/global_optimization/#Global-Optimization-via-NLopt","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"","category":"section"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"The build_loss_objective function builds an objective function compatible with MathOptInterface-associated solvers. This includes packages like IPOPT, NLopt, MOSEK, etc. Building off of the previous example, we can build a cost function for the single parameter optimization problem like:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"using DifferentialEquations, Plots, DiffEqParamEstim, Optimization, OptimizationMOI,\n      OptimizationNLopt, NLopt\n\nfunction f(du, u, p, t)\n    du[1] = p[1] * u[1] - u[1] * u[2]\n    du[2] = -3 * u[2] + u[1] * u[2]\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5]\nprob = ODEProblem(f, u0, tspan, p)\nsol = solve(prob, Tsit5())\n\nt = collect(range(0, stop = 10, length = 200))\nrandomized = VectorOfArray([(sol(t[i]) + 0.01randn(2)) for i in 1:length(t)])\ndata = convert(Array, randomized)\n\nobj = build_loss_objective(prob, Tsit5(), L2Loss(t, data), Optimization.AutoForwardDiff())","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"You can either use the NLopt package directly or through either the OptimizationNLopt or OptimizationMOI which provides an interface for all MathOptInterface compatible non-linear solvers.","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can now use this obj as the objective function with MathProgBase solvers. For our example, we will use NLopt. To use the local derivative-free Constrained Optimization BY Linear Approximations algorithm, we can simply do:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:LN_COBYLA, 1)\noptprob = Optimization.OptimizationProblem(obj, [1.3])\nres = solve(optprob, opt)","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"For a modified evolutionary algorithm, we can use:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:GN_ESCH, 1)\nlower_bounds!(opt, [0.0])\nupper_bounds!(opt, [5.0])\nxtol_rel!(opt, 1e-3)\nmaxeval!(opt, 100000)\nres = solve(optprob, opt)","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"We can even use things like the Improved Stochastic Ranking Evolution Strategy (and add constraints if needed). Let's use this through OptimizationMOI:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"optprob = Optimization.OptimizationProblem(obj, [0.2], lb = [-1.0], ub = [5.0])\nres = solve(optprob,\n    OptimizationMOI.MOI.OptimizerWithAttributes(NLopt.Optimizer,\n        \"algorithm\" => :GN_ISRES,\n        \"xtol_rel\" => 1e-3,\n        \"maxeval\" => 10000))","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"which is very robust to the initial condition. We can also directly use the NLopt interface as below. The fastest result comes from the following algorithm choice:","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"opt = Opt(:LN_BOBYQA, 1)\nmin_objective!(opt, obj)\n(minf,minx,ret) = NLopt.optimize(opt,[1.3])","category":"page"},{"location":"tutorials/global_optimization/","page":"Global Optimization via NLopt","title":"Global Optimization via NLopt","text":"For more information, see the NLopt documentation for more details. And give IPOPT or MOSEK a try!","category":"page"},{"location":"tutorials/stochastic_evaluations/#Parameter-Estimation-for-Stochastic-Differential-Equations-and-Ensembles","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"","category":"section"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We can use any DEProblem, which not only includes DAEProblem and DDEProblems, but also stochastic problems. In this case, let's use the generalized maximum likelihood to fit the parameters of an SDE's ensemble evaluation.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Let's use the same Lotka-Volterra equation as before, but this time add noise:","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"using DifferentialEquations, DiffEqParamEstim, Plots, Optimization, ForwardDiff,\n      OptimizationOptimJL\n\npf_func = function (du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3 * u[2] + u[1] * u[2]\nend\n\nu0 = [1.0; 1.0]\ntspan = (0.0, 10.0)\np = [1.5, 1.0]\npg_func = function (du, u, p, t)\n    du[1] = 1e-6u[1]\n    du[2] = 1e-6u[2]\nend\nprob = SDEProblem(pf_func, pg_func, u0, tspan, p)\nsol = solve(prob, SRIW1())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now let's generate a dataset from 10,000 solutions of the SDE","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"using RecursiveArrayTools # for VectorOfArray\nt = collect(range(0, stop = 10, length = 200))\nfunction generate_data(t)\n    sol = solve(prob, SRIW1())\n    randomized = VectorOfArray([(sol(t[i]) + 0.01randn(2)) for i in 1:length(t)])\n    data = convert(Array, randomized)\nend\naggregate_data = convert(Array, VectorOfArray([generate_data(t) for i in 1:10000]))","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Now let's estimate the parameters. Instead of using single runs from the SDE, we will use a EnsembleProblem. This means that it will solve the SDE N times to come up with an approximate probability distribution at each time point and use that in the likelihood estimate.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"monte_prob = EnsembleProblem(prob)","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"We use Optim.jl for optimization below","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"obj = build_loss_objective(monte_prob, SOSRI(), L2Loss(t, aggregate_data),\n    Optimization.AutoForwardDiff(),\n    maxiters = 10000, verbose = false, trajectories = 1000)\noptprob = Optimization.OptimizationProblem(obj, [1.0, 0.5])\nresult = solve(optprob, Optim.BFGS())","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Parameter Estimation in case of SDE's with a regular L2Loss can have poor accuracy due to only fitting against the mean properties as mentioned in First Differencing.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"result.original","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Instead, when we use L2Loss with first differencing enabled, we get much more accurate estimates.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"obj = build_loss_objective(monte_prob, SRIW1(),\n    L2Loss(t, aggregate_data, differ_weight = 1.0,\n        data_weight = 0.5), Optimization.AutoForwardDiff(),\n    verbose = false, trajectories = 1000, maxiters = 1000)\noptprob = Optimization.OptimizationProblem(obj, [1.0, 0.5])\nresult = solve(optprob, Optim.BFGS())\nresult.original","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"Here, we see that we successfully recovered the drift parameter, and got close to the original noise parameter after searching a two-orders-of-magnitude range.","category":"page"},{"location":"tutorials/stochastic_evaluations/","page":"Parameter Estimation for Stochastic Differential Equations and Ensembles","title":"Parameter Estimation for Stochastic Differential Equations and Ensembles","text":"println(result.u)","category":"page"},{"location":"methods/recommended_methods/#Recommended-Methods","page":"Recommended Methods","title":"Recommended Methods","text":"","category":"section"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"The recommended method is to use build_loss_objective with the optimizer of your choice. This method can thus be paired with global optimizers from packages like BlackBoxOptim.jl or NLopt.jl which can be much less prone to finding local minima than local optimization methods. Also, it allows the user to define the cost function in the way they choose as a function loss(sol). This package can thus fit using any cost function on the solution, making it applicable to fitting non-temporal data and other types of problems. Also, build_loss_objective works for all the DEProblem types, allowing it to optimize parameters on ODEs, SDEs, DDEs, DAEs, etc.","category":"page"},{"location":"methods/recommended_methods/","page":"Recommended Methods","title":"Recommended Methods","text":"However, this method requires repeated solution of the differential equation. If the data is temporal data, the most efficient method is the two_stage_objective which does not require repeated solutions but is not as accurate. Usage of the two_stage_objective should have a post-processing step which refines using a method like build_loss_objective.","category":"page"},{"location":"methods/optimization_based_methods/#Optimization-Based-Methods","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Objective-Function-Builders","page":"Optimization-Based Methods","title":"The Objective Function Builders","text":"","category":"section"},{"location":"methods/optimization_based_methods/#Standard-Nonlinear-Regression","page":"Optimization-Based Methods","title":"Standard Nonlinear Regression","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"build_loss_objective builds an objective function to be used with Optim.jl and MathProgBase-associated solvers like NLopt.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function build_loss_objective(prob::DEProblem, alg, loss,\n        adtype = SciMLBase.NoAD(),\n        regularization = nothing;\n        priors = nothing,\n        prob_generator = STANDARD_PROB_GENERATOR,\n        kwargs...)\nend","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The first argument is the DEProblem to solve, and next is the alg to use. The alg must match the problem type, which can be any DEProblem (ODEs, SDEs, DAEs, DDEs, etc.). regularization defaults to nothing, which has no regularization function. The extra keyword arguments are passed to the differential equation solver.","category":"page"},{"location":"methods/optimization_based_methods/#Multiple-Shooting","page":"Optimization-Based Methods","title":"Multiple Shooting","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Multiple Shooting is often used in Boundary Value Problems (BVP) and is more robust than the regular objective function used in these problems. It proceeds as follows:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Divide up the time span into short time periods and solve the equation with the current parameters which here consist of both, the parameters of the differential equations and also the initial values for the short time periods.\nThis objective additionally involves a discontinuity error term that imposes higher cost if the end of the solution of one time period doesn't match the beginning of the next one.\nMerge the solutions from the shorter intervals and then calculate the loss.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function multiple_shooting_objective(prob::DiffEqBase.DEProblem, alg, loss,\n        adtype = SciMLBase.NoAD(),\n        regularization = nothing;\n        priors = nothing,\n        discontinuity_weight = 1.0,\n        prob_generator = STANDARD_PROB_GENERATOR,\n        kwargs...)\nend","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For consistency multiple_shooting_objective takes exactly the same arguments as build_loss_objective. It also has the option for discontinuity_weight as a keyword argument, which assigns weight to the error occurring due to the discontinuity that arises from the breaking up of the time span.","category":"page"},{"location":"methods/optimization_based_methods/#Detailed-Explanations-of-Arguments","page":"Optimization-Based Methods","title":"Detailed Explanations of Arguments","text":"","category":"section"},{"location":"methods/optimization_based_methods/#The-Loss-Function","page":"Optimization-Based Methods","title":"The Loss Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"loss(sol)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"is the function, which reduces the problem's solution to a scalar, which the optimizer will try to minimize. While this is very flexible, two convenience routines are included for fitting to data with standard cost functions:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t, data; differ_weight = nothing, data_weight = nothing,\n    colloc_grad = nothing, dudt = nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"where t is the set of timepoints which the data are found at, and data are the values that are known where each column corresponds to measures of the values of the system. L2Loss is an optimized version of the L2-distance. The data_weight is a scalar or vector of weights for the loss function which must match the size of the data. Note that minimization of a weighted L2Loss is equivalent to maximum likelihood estimation of a heteroskedastic Normally distributed likelihood. differ_weight allows one to add a weight on the first differencing terms sol[i+1]-sol[i] against the data first differences. This smooths out the loss term and can make it easier to fit strong solutions of stochastic models, but is zero (nothing) by default. Additionally, colloc_grad allows one to give a matrix of the collocation gradients for the data. This is used to add an interpolation derivative term, like the two-stage method. A convenience function colloc_grad(t,data) returns a collocation gradient from a 3rd order spline calculated by Dierckx.jl, which can be used as the colloc_grad. Note that, with a collocation gradient and regularization, this loss is equivalent to a 4DVAR.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Additionally, we include a more flexible log-likelihood approach:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"LogLikeLoss(t, distributions, diff_distributions = nothing)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"In this case, there are two forms. The simple case is where distributions[i,j] is the likelihood distributions from a UnivariateDistribution from Distributions.jl, where it corresponds to the likelihood at t[i] for component j. The second case is where distributions[i] is a MultivariateDistribution which corresponds to the likelihood at t[i] over the vector of components. This likelihood function then calculates the negative of the total log-likelihood over time as its objective value (negative since optimizers generally find minimums, and thus this corresponds to maximum likelihood estimation). The third term, diff_distributions, acts similarly but allows putting a distribution on the first difference terms sol[i+1]-sol[i].","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Note that these distributions can be generated via fit_mle on some dataset against some chosen distribution type.","category":"page"},{"location":"methods/optimization_based_methods/#Note-About-Loss-Functions","page":"Optimization-Based Methods","title":"Note About Loss Functions","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"For parameter estimation problems, it's not uncommon for the optimizers to hit unstable regions of parameter space. This causes warnings that the solver exited early, and the built-in loss functions like L2Loss automatically handle this. However, if using a user-supplied loss function, you should make sure it's robust to these issues. One common pattern is to apply infinite loss when the integration is not successful. Using the retcodes, this can be done via:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"function my_loss_function(sol)\n    tot_loss = 0.0\n    if any((!SciMLBase.successful_retcode(s.retcode) for s in sol))\n        tot_loss = Inf\n    else\n        # calculation for the loss here\n    end\n    tot_loss\nend","category":"page"},{"location":"methods/optimization_based_methods/#fd","page":"Optimization-Based Methods","title":"Note on First Differencing","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"L2Loss(t, data, differ_weight = 0.3, data_weight = 0.7)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"First differencing incorporates the differences of data points at consecutive time points which adds more information about the trajectory in the loss function. Adding first differencing is helpful in cases where the L2Loss alone leads to non-identifiable parameters, but adding a first differencing term makes it more identifiable. This can be noted on stochastic differential equation models, where this aims to capture the autocorrelation and therefore helps us avoid getting the same stationary distribution despite different trajectories and thus wrong parameter estimates.","category":"page"},{"location":"methods/optimization_based_methods/#The-Regularization-Function","page":"Optimization-Based Methods","title":"The Regularization Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization can be any function of p, the parameter vector:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"regularization(p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The Regularization helper function builds a regularization using a penalty function penalty from PenaltyFunctions.jl:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"reg = Regularization(λ, penalty = L2Penalty())\nbuild_loss_objective(prob, alg, loss, SciMLBase.NoAD(), reg)\n\nusing Optimization, Zygote\nbuild_loss_objective(prob, alg, loss, Optimization.AutoZygote(), reg)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The regularization defaults to L2 if no penalty function is specified. λ is the weight parameter for the addition of the regularization term.","category":"page"},{"location":"methods/optimization_based_methods/#Using-automatic-differentiation","page":"Optimization-Based Methods","title":"Using automatic differentiation","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"To use derivatives with optimization solvers, Optimization.jl's adtype argument as described here should be used with the wrapper subpackage OptimizationOptimJL, OptimizationNLopt etc.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"using Optimization, ForwardDiff\nbuild_loss_objective(prob, alg, loss, Optimization.AutoForwardDiff())\nmultiple_shooting_objective(prob, alg, loss, Optimization.AutoForwardDiff())","category":"page"},{"location":"methods/optimization_based_methods/#The-Problem-Generator-Function","page":"Optimization-Based Methods","title":"The Problem Generator Function","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"The argument prob_generator allows one to specify a function for generating new problems from a given parameter set. By default, this just builds a new problem which fixes the element types in a way that's autodifferentiation compatible and adds the new parameter vector p. For example, the code for this is:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob, p) -> remake(prob, u0 = convert.(eltype(p), prob.u0), p = p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"Then the new problem with these new values is returned.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"One can use this to change the meaning of the parameters using this function. For example, if one instead wanted to optimize the initial conditions for a function without parameters, you could change this to:","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"prob_generator = (prob, p) -> remake(prob.f, u0 = p)","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"which simply uses p as the initial condition in the initial value problem.","category":"page"},{"location":"methods/optimization_based_methods/#Using-the-Objectives-for-MAP-estimates","page":"Optimization-Based Methods","title":"Using the Objectives for MAP estimates","text":"","category":"section"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"You can also add a prior option to build_loss_objective and multiple_shooting_objective that essentially turns it into MAP by multiplying the log-likelihood (the cost) by the prior. The option is available as the keyword argument priors, it can take in either an array of univariate distributions for each of the parameters or a multivariate distribution.","category":"page"},{"location":"methods/optimization_based_methods/","page":"Optimization-Based Methods","title":"Optimization-Based Methods","text":"ms_obj = multiple_shooting_objective(ms_prob, Tsit5(), L2Loss(t, data); priors = priors,\n    discontinuity_weight = 1.0, abstol = 1e-12,\n    reltol = 1e-12)","category":"page"},{"location":"#DiffEqParamEstim.jl:-Parameter-Estimation-for-Differential-Equations","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"DiffEqParamEstim.jl is a package for simplified parameter estimation with DifferentialEquations.jl While not as expansive as SciMLSensitivity.jl, it's provides a simple interface for users who want to quickly run standard parameter estimation routines for model calibration on not too large of models (<100 parameters or ODEs). It is designed to integrate with Optimization.jl interface or directly use with an optimization package.","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"note: Note\nFor much larger models and more complex setups (multiple datasets, batching, etc.) see SciMLSensitivity.","category":"page"},{"location":"#Installation","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Installation","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"To install DiffEqParamEstim.jl, use the Julia package manager:","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg\nPkg.add(\"DiffEqParamEstim\")","category":"page"},{"location":"#Contributing","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Contributing","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Reproducibility","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"</details>","category":"page"},{"location":"","page":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","title":"DiffEqParamEstim.jl: Parameter Estimation for Differential Equations","text":"using TOML\nusing Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"page"},{"location":"tutorials/ensemble/#Fitting-Ensembles-of-ODE-Models-to-Data","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"","category":"section"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"In this tutorial, we will showcase how to fit multiple models simultaneously to respective data sources. Let's dive right in!","category":"page"},{"location":"tutorials/ensemble/#Formulating-the-Ensemble-Model","page":"Fitting Ensembles of ODE Models to Data","title":"Formulating the Ensemble Model","text":"","category":"section"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"First, you want to create a problem which solves multiple problems at the same time. This is the EnsembleProblem. When the parameter estimation tools say it will take any DEProblem, it really means ANY DEProblem, which includes EnsembleProblem.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"So, let's get an EnsembleProblem setup that solves with 10 different initial conditions. This looks as follows:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"using DifferentialEquations, DiffEqParamEstim, Plots, Optimization, ForwardDiff,\n      OptimizationOptimJL\n\n# Monte Carlo Problem Set Up for solving set of ODEs with different initial conditions\n\n# Set up Lotka-Volterra system\nfunction pf_func(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3 * u[2] + u[1] * u[2]\nend\np = [1.5, 1.0]\nprob = ODEProblem(pf_func, [1.0, 1.0], (0.0, 10.0), p)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now for an EnsembleProblem we have to take this problem and tell it what to do N times via the prob_func. So let's generate N=10 different initial conditions, and tell it to run the same problem but with these 10 different initial conditions each time:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Setting up to solve the problem N times (for the N different initial conditions)\nN = 10;\ninitial_conditions = [\n    [1.0, 1.0],\n    [1.0, 1.5],\n    [1.5, 1.0],\n    [1.5, 1.5],\n    [0.5, 1.0],\n    [1.0, 0.5],\n    [0.5, 0.5],\n    [2.0, 1.0],\n    [1.0, 2.0],\n    [2.0, 2.0]\n]\nfunction prob_func(prob, i, repeat)\n    ODEProblem(prob.f, initial_conditions[i], prob.tspan, prob.p)\nend\nenprob = EnsembleProblem(prob, prob_func = prob_func)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"We can check this does what we want by solving it:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Check above does what we want\nsim = solve(enprob, Tsit5(), trajectories = N)\nplot(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"trajectories=N means “run N times”, and each time it runs the problem returned by the prob_func, which is always the same problem but with the ith initial condition.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now let's generate a dataset from that. Let's get data points at every t=0.1 using saveat, and then convert the solution into an array.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Generate a dataset from these runs\ndata_times = 0.0:0.1:10.0\nsim = solve(enprob, Tsit5(), trajectories = N, saveat = data_times)\ndata = Array(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Here, data[i,j,k] is the same as sim[i,j,k] which is the same as sim[k][i,j] (where sim[k] is the kth solution). So data[i,j,k] is the jth timepoint of the ith variable in the kth trajectory.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Now let's build a loss function. A loss function is some loss(sol) that spits out a scalar for how far from optimal we are. In the documentation, I show that we normally do loss = L2Loss(t,data), but we can bootstrap off of this. Instead, let's build an array of N loss functions, each one with the correct piece of data.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"# Building a loss function\nlosses = [L2Loss(data_times, data[:, :, i]) for i in 1:N]","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"So losses[i] is a function which computes the loss of a solution against the data of the ith trajectory. So to build our true loss function, we sum the losses:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"loss(sim) = sum(losses[i](sim[i]) for i in 1:N)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"As a double check, make sure that loss(sim) outputs zero (since we generated the data from sim). Now we generate data with other parameters:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"prob = ODEProblem(pf_func, [1.0, 1.0], (0.0, 10.0), [1.2, 0.8])\nfunction prob_func(prob, i, repeat)\n    ODEProblem(prob.f, initial_conditions[i], prob.tspan, prob.p)\nend\nenprob = EnsembleProblem(prob, prob_func = prob_func)\nsim = solve(enprob, Tsit5(), trajectories = N, saveat = data_times)\nloss(sim)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"and get a non-zero loss. So, we now have our problem, our data, and our loss function… we have what we need.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Put this into buildlossobjective.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"obj = build_loss_objective(enprob, Tsit5(), loss, Optimization.AutoForwardDiff(),\n    trajectories = N,\n    saveat = data_times)","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Notice that we added the kwargs for solve of the EnsembleProblem into this. They get passed to the internal solve command, so then the loss is computed on N trajectories at data_times.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Thus, we take this objective function over to any optimization package. Here, since the Lotka-Volterra equation requires positive parameters, we use Fminbox to make sure the parameters stay within the passed bounds. Let's start the optimization with [1.3,0.9], Optim spits out that the true parameters are:","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"lower = zeros(2)\nupper = fill(2.0, 2)\noptprob = OptimizationProblem(obj, [1.3, 0.9], lb = lower, ub = upper)\nresult = solve(optprob, Fminbox(BFGS()))","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"result","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"Optim finds one but not the other parameter.","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"It is advised to run a test on synthetic data for your problem before using it on real data. Maybe play around with different optimization packages, or add regularization. You may also want to decrease the tolerance of the ODE solvers via","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"obj = build_loss_objective(enprob, Tsit5(), loss, Optimization.AutoForwardDiff(),\n    trajectories = N,\n    abstol = 1e-8, reltol = 1e-8,\n    saveat = data_times)\noptprob = OptimizationProblem(obj, [1.3, 0.9], lb = lower, ub = upper)\nresult = solve(optprob, BFGS()) #OptimizationOptimJL detects that it's a box constrained problem and use Fminbox wrapper over BFGS","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"result","category":"page"},{"location":"tutorials/ensemble/","page":"Fitting Ensembles of ODE Models to Data","title":"Fitting Ensembles of ODE Models to Data","text":"if you suspect error is the problem. However, if you're having problems it's most likely not the ODE solver tolerance and mostly because parameter inference is a very hard optimization problem.","category":"page"}]
}
