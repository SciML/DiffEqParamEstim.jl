<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Getting Started with Optimization-Based ODE Parameter Estimation · DiffEqParamEstim.jl</title><meta name="title" content="Getting Started with Optimization-Based ODE Parameter Estimation · DiffEqParamEstim.jl"/><meta property="og:title" content="Getting Started with Optimization-Based ODE Parameter Estimation · DiffEqParamEstim.jl"/><meta property="twitter:title" content="Getting Started with Optimization-Based ODE Parameter Estimation · DiffEqParamEstim.jl"/><meta name="description" content="Documentation for DiffEqParamEstim.jl."/><meta property="og:description" content="Documentation for DiffEqParamEstim.jl."/><meta property="twitter:description" content="Documentation for DiffEqParamEstim.jl."/><meta property="og:url" content="https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/"/><meta property="twitter:url" content="https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/"/><link rel="canonical" href="https://docs.sciml.ai/DiffEqParamEstim/stable/getting_started/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="DiffEqParamEstim.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">DiffEqParamEstim.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">DiffEqParamEstim.jl: Parameter Estimation for Differential Equations</a></li><li class="is-active"><a class="tocitem" href>Getting Started with Optimization-Based ODE Parameter Estimation</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#Required-Dependencies"><span>Required Dependencies</span></a></li><li><a class="tocitem" href="#Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case"><span>Parameter Estimation in the Lotka-Volterra Equation: 1 Parameter Case</span></a></li><li><a class="tocitem" href="#Estimating-Multiple-Parameters-Simultaneously"><span>Estimating Multiple Parameters Simultaneously</span></a></li><li><a class="tocitem" href="#Conclusion"><span>Conclusion</span></a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/global_optimization/">Global Optimization via NLopt</a></li><li><a class="tocitem" href="../tutorials/generalized_likelihood/">Generalized Likelihood Inference</a></li><li><a class="tocitem" href="../tutorials/stochastic_evaluations/">Parameter Estimation for Stochastic Differential Equations and Ensembles</a></li><li><a class="tocitem" href="../tutorials/ensemble/">Fitting Ensembles of ODE Models to Data</a></li></ul></li><li><span class="tocitem">Methods</span><ul><li><a class="tocitem" href="../methods/recommended_methods/">Recommended Methods</a></li><li><a class="tocitem" href="../methods/optimization_based_methods/">Optimization-Based Methods</a></li><li><a class="tocitem" href="../methods/collocation_loss/">Two Stage method (Non-Parametric Collocation)</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Getting Started with Optimization-Based ODE Parameter Estimation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Getting Started with Optimization-Based ODE Parameter Estimation</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/SciML/DiffEqParamEstim.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/SciML/DiffEqParamEstim.jl/blob/master/docs/src/getting_started.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Getting-Started-with-Optimization-Based-ODE-Parameter-Estimation"><a class="docs-heading-anchor" href="#Getting-Started-with-Optimization-Based-ODE-Parameter-Estimation">Getting Started with Optimization-Based ODE Parameter Estimation</a><a id="Getting-Started-with-Optimization-Based-ODE-Parameter-Estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Started-with-Optimization-Based-ODE-Parameter-Estimation" title="Permalink"></a></h1><p>In this tutorial, we will showcase how to estimate the parameters of an ordinary differential equation using DiffEqParamEstim.jl. DiffEqParamEstim.jl is a high-level tool that makes common parameter estimation tasks simple. Here, we will show how to use its cost function generation to estimate the parameters of the Lotka-Volterra equation against simulated data.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>First, we will make sure DiffEqParamEstim.jl is installed correctly. To do this, we use the Julia package REPL, opened by typing <code>]</code> in the REPL and seeing <code>pkg&gt;</code> appear in blue. Then we type: <code>add DiffEqParamEstim</code> and hit enter. This will run the package management sequence, and we will be good to go.</p><h2 id="Required-Dependencies"><a class="docs-heading-anchor" href="#Required-Dependencies">Required Dependencies</a><a id="Required-Dependencies-1"></a><a class="docs-heading-anchor-permalink" href="#Required-Dependencies" title="Permalink"></a></h2><table><tr><th style="text-align: left">Module</th><th style="text-align: left">Description</th></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/DiffEqDocs/stable/">DifferentialEquations.jl</a></td><td style="text-align: left">The numerical differential equation solver package</td></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/RecursiveArrayTools/stable/">RecursiveArrayTools.jl</a></td><td style="text-align: left">Tooling for recursive arrays like vector of arrays</td></tr><tr><td style="text-align: left"><a href="https://docs.juliaplots.org/latest/">Plots.jl</a></td><td style="text-align: left">Tooling for plotting and visualization</td></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/Zygote/stable/">Zygote.jl</a></td><td style="text-align: left">Tooling for reverse-mode automatic differentiation (gradient calculations)</td></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/Optimization/stable/">Optimization.jl</a></td><td style="text-align: left">The numerical optimization package</td></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/Optimization/stable/optimization_packages/optim/">OptimizationOptimJL.jl</a></td><td style="text-align: left">The Optim optimizers we will use for local optimization</td></tr><tr><td style="text-align: left"><a href="https://docs.sciml.ai/Optimization/stable/optimization_packages/blackboxoptim/">OptimizationBBO.jl</a></td><td style="text-align: left">The BlackBoxOptim optimizers we will use for global optimization</td></tr></table><h2 id="Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case"><a class="docs-heading-anchor" href="#Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case">Parameter Estimation in the Lotka-Volterra Equation: 1 Parameter Case</a><a id="Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Estimation-in-the-Lotka-Volterra-Equation:-1-Parameter-Case" title="Permalink"></a></h2><p>We choose to optimize the parameters on the Lotka-Volterra equation. Let&#39;s start by defining the equation as a function with parameters:</p><pre><code class="language-julia hljs">using DifferentialEquations, RecursiveArrayTools, Plots, DiffEqParamEstim
using Optimization, ForwardDiff, OptimizationOptimJL, OptimizationBBO

function f(du, u, p, t)
    du[1] = dx = p[1] * u[1] - u[1] * u[2]
    du[2] = dy = -3 * u[2] + u[1] * u[2]
end

u0 = [1.0; 1.0]
tspan = (0.0, 10.0)
p = [1.5]
prob = ODEProblem(f, u0, tspan, p)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr38_2" style="color:#56b6c2">ODEProblem</span> with uType <span class="sgr38_2" style="color:#56b6c2">Vector{Float64}</span> and tType <span class="sgr38_2" style="color:#56b6c2">Float64</span>. In-place: <span class="sgr38_2" style="color:#56b6c2">true</span>
timespan: (0.0, 10.0)
u0: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><h3 id="Generating-Synthetic-Data"><a class="docs-heading-anchor" href="#Generating-Synthetic-Data">Generating Synthetic Data</a><a id="Generating-Synthetic-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-Synthetic-Data" title="Permalink"></a></h3><p>We create data using the numerical result with <code>a=1.5</code>:</p><pre><code class="language-julia hljs">sol = solve(prob, Tsit5())
t = collect(range(0, stop = 10, length = 200))
using RecursiveArrayTools # for VectorOfArray
randomized = VectorOfArray([(sol(t[i]) + 0.01randn(2)) for i in 1:length(t)])
data = convert(Array, randomized)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2×200 Matrix{Float64}:
 0.98279  1.02535   1.06368   1.07795   …  0.98301  0.989753  1.03331
 1.00582  0.906602  0.833809  0.731948     1.11779  0.989833  0.887986</code></pre><p>Here, we used <code>VectorOfArray</code> from <a href="https://docs.sciml.ai/RecursiveArrayTools/stable/">RecursiveArrayTools.jl</a> to turn the result of an ODE into a matrix.</p><p>If we plot the solution with the parameter at <code>a=1.42</code>, we get the following:</p><pre><code class="language-julia hljs">newprob = remake(prob, p = [1.42])
newsol = solve(newprob, Tsit5())
plot(sol)
plot!(newsol)</code></pre><img src="0d48eab0.svg" alt="Example block output"/><p>Notice that after one period, this solution begins to drift very far off: this problem is sensitive to the choice of <code>a</code>.</p><p>To build the objective function for Optim.jl, we simply call the <code>build_loss_objective</code> function:</p><pre><code class="language-julia hljs">cost_function = build_loss_objective(prob, Tsit5(), L2Loss(t, data),
    Optimization.AutoForwardDiff(),
    maxiters = 10000, verbose = false)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::SciMLBase.OptimizationFunction{true, ADTypes.AutoForwardDiff{nothing, Nothing}, DiffEqParamEstim.var&quot;#29#30&quot;{Nothing, typeof(DiffEqParamEstim.STANDARD_PROB_GENERATOR), Base.Pairs{Symbol, Integer, Tuple{Symbol, Symbol}, @NamedTuple{maxiters::Int64, verbose::Bool}}, SciMLBase.ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, SciMLBase.ODEFunction{true, SciMLBase.AutoSpecialize, typeof(Main.f), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}, SciMLBase.StandardODEProblem}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, L2Loss{Vector{Float64}, Matrix{Float64}, Nothing, Nothing, Nothing}, Nothing, Tuple{}}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}) (generic function with 1 method)</code></pre><p>This objective function internally is calling the ODE solver to get solutions to test against the data. The keyword arguments are passed directly to the solver. Note that we set <code>maxiters</code> in a way that causes the differential equation solvers to error more quickly when in bad regions of the parameter space, speeding up the process. If the integrator stops early (due to divergence), then those parameters are given an infinite loss, and thus this is a quick way to avoid bad parameters. We set <code>verbose=false</code> because this divergence can get noisy. The <code>Optimization.AutoForwardDiff()</code> is a choice of automatic differentiation, i.e., how the gradients are calculated. For more information on this choice, see <a href="https://docs.sciml.ai/Optimization/stable/API/optimization_function/#Automatic-Differentiation-Construction-Choice-Recommendations">the automatic differentiation choice API</a>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>A good rule of thumb is to use <code>Optimization.AutoForwardDiff()</code> for less than 100 parameters + states, and <code>Optimization.AutoZygote()</code> for more.</p></div></div><p>Before optimizing, let&#39;s visualize our cost function by plotting it for a range of parameter values:</p><pre><code class="language-julia hljs">vals = 0.0:0.1:10.0
plot(vals, [cost_function(i) for i in vals], yscale = :log10,
    xaxis = &quot;Parameter&quot;, yaxis = &quot;Cost&quot;, title = &quot;1-Parameter Cost Function&quot;,
    lw = 3)</code></pre><img src="98db028f.svg" alt="Example block output"/><p>Here we see that there is a very well-defined minimum in our cost function at the real parameter (because this is where the solution almost exactly fits the dataset).</p><p>Now we can use the <code>BFGS</code> algorithm to optimize the parameter starting at <code>a=1.42</code>. We do this by creating an optimization problem and solving that with <code>BFGS()</code>:</p><pre><code class="language-julia hljs">optprob = Optimization.OptimizationProblem(cost_function, [1.42])
optsol = solve(optprob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 1-element Vector{Float64}:
 1.500444215916464</code></pre><p>Now let&#39;s see how well the fit performed:</p><pre><code class="language-julia hljs">newprob = remake(prob, p = optsol.u)
newsol = solve(newprob, Tsit5())
plot(sol)
plot!(newsol)</code></pre><img src="1199e86b.svg" alt="Example block output"/><p>Note that some algorithms may be sensitive to the initial condition. For more details on using Optim.jl, see the <a href="https://julianlsolvers.github.io/Optim.jl/stable/">documentation for Optim.jl</a>.</p><h3 id="Adding-Bounds-Constraints"><a class="docs-heading-anchor" href="#Adding-Bounds-Constraints">Adding Bounds Constraints</a><a id="Adding-Bounds-Constraints-1"></a><a class="docs-heading-anchor-permalink" href="#Adding-Bounds-Constraints" title="Permalink"></a></h3><p>We can improve our solution by noting that the Lotka-Volterra equation requires that the parameters are positive. Thus, <a href="https://docs.sciml.ai/Optimization/stable/API/optimization_problem/">following the Optimization.jl documentation</a> we can add box constraints to ensure the optimizer only checks between <code>0.0</code> and <code>3.0</code> which improves the efficiency of our algorithm. We pass the <code>lb</code> and <code>ub</code> keyword arguments to the <code>OptimizationProblem</code> to pass these bounds to the optimizer:</p><pre><code class="language-julia hljs">lower = [0.0]
upper = [3.0]
optprob = Optimization.OptimizationProblem(cost_function, [1.42], lb = lower, ub = upper)
result = solve(optprob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 1-element Vector{Float64}:
 1.5004448974034192</code></pre><h2 id="Estimating-Multiple-Parameters-Simultaneously"><a class="docs-heading-anchor" href="#Estimating-Multiple-Parameters-Simultaneously">Estimating Multiple Parameters Simultaneously</a><a id="Estimating-Multiple-Parameters-Simultaneously-1"></a><a class="docs-heading-anchor-permalink" href="#Estimating-Multiple-Parameters-Simultaneously" title="Permalink"></a></h2><p>Lastly, we can use the same tools to estimate multiple parameters simultaneously. Let&#39;s use the Lotka-Volterra equation with all parameters free:</p><pre><code class="language-julia hljs">function f2(du, u, p, t)
    du[1] = dx = p[1] * u[1] - p[2] * u[1] * u[2]
    du[2] = dy = -p[3] * u[2] + p[4] * u[1] * u[2]
end

u0 = [1.0; 1.0]
tspan = (0.0, 10.0)
p = [1.5, 1.0, 3.0, 1.0]
prob = ODEProblem(f2, u0, tspan, p)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi"><span class="sgr38_2" style="color:#56b6c2">ODEProblem</span> with uType <span class="sgr38_2" style="color:#56b6c2">Vector{Float64}</span> and tType <span class="sgr38_2" style="color:#56b6c2">Float64</span>. In-place: <span class="sgr38_2" style="color:#56b6c2">true</span>
timespan: (0.0, 10.0)
u0: 2-element Vector{Float64}:
 1.0
 1.0</code></pre><p>We can build an objective function and solve the multiple parameter version just as before:</p><pre><code class="language-julia hljs">cost_function = build_loss_objective(prob, Tsit5(), L2Loss(t, data),
    Optimization.AutoForwardDiff(),
    maxiters = 10000, verbose = false)
optprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])
result_bfgs = solve(optprob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 4-element Vector{Float64}:
 1.5008428359972275
 1.001237270157027
 2.998137484852291
 0.9998740716338155</code></pre><h3 id="Alternative-Cost-Functions-for-Increased-Robustness"><a class="docs-heading-anchor" href="#Alternative-Cost-Functions-for-Increased-Robustness">Alternative Cost Functions for Increased Robustness</a><a id="Alternative-Cost-Functions-for-Increased-Robustness-1"></a><a class="docs-heading-anchor-permalink" href="#Alternative-Cost-Functions-for-Increased-Robustness" title="Permalink"></a></h3><p>The <code>build_loss_objective</code> with <code>L2Loss</code> is the most naive approach for parameter estimation. There are many others.</p><p>We can also use First-Differences in L2Loss by passing the kwarg <code>differ_weight</code> which decides the contribution of the differencing loss to the total loss.</p><pre><code class="language-julia hljs">cost_function = build_loss_objective(prob, Tsit5(),
    L2Loss(t, data, differ_weight = 0.3,
        data_weight = 0.7),
    Optimization.AutoForwardDiff(),
    maxiters = 10000, verbose = false)
optprob = Optimization.OptimizationProblem(cost_function, [1.3, 0.8, 2.8, 1.2])
result_bfgs = solve(optprob, BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 4-element Vector{Float64}:
 1.5015730184543454
 1.0013326052323224
 2.995671104897097
 0.9990137601172969</code></pre><p>We can also use Multiple Shooting method by creating a <code>multiple_shooting_objective</code></p><pre><code class="language-julia hljs">function ms_f1(du, u, p, t)
    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]
    du[2] = -3.0 * u[2] + u[1] * u[2]
end
ms_u0 = [1.0; 1.0]
tspan = (0.0, 10.0)
ms_p = [1.5, 1.0]
ms_prob = ODEProblem(ms_f1, ms_u0, tspan, ms_p)
t = collect(range(0, stop = 10, length = 200))
data = Array(solve(ms_prob, Tsit5(), saveat = t, abstol = 1e-12, reltol = 1e-12))
bound = Tuple{Float64, Float64}[(0, 10), (0, 10), (0, 10), (0, 10),
    (0, 10), (0, 10), (0, 10), (0, 10),
    (0, 10), (0, 10), (0, 10), (0, 10),
    (0, 10), (0, 10), (0, 10), (0, 10), (0, 10), (0, 10)]

ms_obj = multiple_shooting_objective(ms_prob, Tsit5(), L2Loss(t, data),
    Optimization.AutoForwardDiff();
    discontinuity_weight = 1.0, abstol = 1e-12,
    reltol = 1e-12)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(::SciMLBase.OptimizationFunction{true, ADTypes.AutoForwardDiff{nothing, Nothing}, DiffEqParamEstim.var&quot;#43#48&quot;{Nothing, Float64, DiffEqParamEstim.var&quot;#1#2&quot;, Base.Pairs{Symbol, Float64, Tuple{Symbol, Symbol}, @NamedTuple{abstol::Float64, reltol::Float64}}, SciMLBase.ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, SciMLBase.ODEFunction{true, SciMLBase.AutoSpecialize, typeof(Main.ms_f1), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing}, Base.Pairs{Symbol, Union{}, Tuple{}, @NamedTuple{}}, SciMLBase.StandardODEProblem}, OrdinaryDiffEq.Tsit5{typeof(OrdinaryDiffEq.trivial_limiter!), typeof(OrdinaryDiffEq.trivial_limiter!), Static.False}, L2Loss{Vector{Float64}, Matrix{Float64}, Nothing, Nothing, Nothing}, Nothing}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED_NO_TIME), Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing}) (generic function with 1 method)</code></pre><p>This creates the objective function that can be passed to an optimizer, from which we can then get the parameter values and the initial values of the short time periods, keeping in mind the indexing. Now we mix this with a global optimization method to improve robustness even more:</p><pre><code class="language-julia hljs">optprob = Optimization.OptimizationProblem(ms_obj, zeros(18), lb = first.(bound),
    ub = last.(bound))
optsol_ms = solve(optprob, BBO_adaptive_de_rand_1_bin_radiuslimited(), maxiters = 10_000)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Failure
u: 18-element Vector{Float64}:
 0.9970360994123162
 1.1052312770332227
 3.563514125876241
 0.24537860268622136
 2.8851388358653334
 4.9768817164548595
 1.3216423693698092
 0.7626906959656669
 4.957841745806615
 0.35980266774808156
 1.5044295894338107
 3.565437612797054
 1.6407500753670192
 0.3689264802343187
 6.2441514430760074
 0.85999846794955
 1.4764526431587193
 0.9625075025217636</code></pre><pre><code class="language-julia hljs">optsol_ms.u[(end - 1):end]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.4764526431587193
 0.9625075025217636</code></pre><p>Here as our model had 2 parameters, we look at the last 2 indexes of <code>result</code> to get our parameter values and the rest of the values are the initial values of the shorter timespans as described in the reference section. We can also use a gradient-based optimizer with the multiple shooting objective.</p><pre><code class="language-julia hljs">optsol_ms = solve(optprob, BFGS())
optsol_ms.u[(end - 1):end]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">2-element Vector{Float64}:
 1.5000000000037266
 1.000000000000063</code></pre><p>The objective function for the Two Stage method can be created and passed to an optimizer as</p><pre><code class="language-julia hljs">two_stage_obj = two_stage_objective(ms_prob, t, data, Optimization.AutoForwardDiff())
optprob = Optimization.OptimizationProblem(two_stage_obj, [1.3, 0.8, 2.8, 1.2])
result = solve(optprob, Optim.BFGS())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">retcode: Success
u: 4-element Vector{Float64}:
 1.5035938533664905
 0.99257311537469
 2.8
 1.2</code></pre><p>The default kernel used in the method is <code>Epanechnikov</code>, available others are <code>Uniform</code>,  <code>Triangular</code>, <code>Quartic</code>, <code>Triweight</code>, <code>Tricube</code>, <code>Gaussian</code>, <code>Cosine</code>, <code>Logistic</code> and <code>Sigmoid</code>, this can be passed by the <code>kernel</code> keyword argument. <code>loss_func</code> keyword argument can be used to pass the loss function (cost function) you want to use and passing a valid <a href="https://docs.sciml.ai/Optimization/stable/getting_started/#Controlling-Gradient-Calculations-(Automatic-Differentiation)"><code>adtype</code> argument</a> enables Auto Differentiation.</p><h2 id="Conclusion"><a class="docs-heading-anchor" href="#Conclusion">Conclusion</a><a id="Conclusion-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusion" title="Permalink"></a></h2><p>There are many more choices for how to improve the robustness of a parameter estimation. With all of these tools, one likely should never do the simple “solve it with <code>p</code> and check the L2 loss”. Instead, we should use these tricks to improve the loss landscape and increase the ability for optimizers to find globally the best parameters.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« DiffEqParamEstim.jl: Parameter Estimation for Differential Equations</a><a class="docs-footer-nextpage" href="../tutorials/global_optimization/">Global Optimization via NLopt »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.2.1 on <span class="colophon-date" title="Thursday 15 February 2024 00:48">Thursday 15 February 2024</span>. Using Julia version 1.10.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
